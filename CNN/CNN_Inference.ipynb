{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea297d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-ner-final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d764834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load vocabularies\n",
    "with open(\"ner_vocab.pkl\", \"rb\") as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "    word2idx = vocab_data[\"word2idx\"]\n",
    "    tag2idx = vocab_data[\"tag2idx\"]\n",
    "    idx2tag = vocab_data[\"idx2tag\"]\n",
    "\n",
    "# Load embedding matrix\n",
    "embedding_matrix = torch.load(\"embedding_matrix.pt\")\n",
    "embed_dim = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f06cd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_NER(\n",
       "  (embedding): Embedding(16028, 100)\n",
       "  (conv1): Conv1d(100, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 900\n",
    "\n",
    "# Re-declare the same model class\n",
    "class CNN_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, embedding_matrix):\n",
    "        super(CNN_NER, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.conv1 = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.dropout(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Reconstruct model and load weights\n",
    "model = CNN_NER(len(word2idx), embed_dim, len(tag2idx), embedding_matrix).to(device)\n",
    "model.load_state_dict(torch.load(\"cnn_ner_model.pt\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aef65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_sentence(raw_text): #predict new sentence\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize input using BERT\n",
    "    tokens = tokenizer.tokenize(raw_text)\n",
    "    \n",
    "    # Map to word2idx (fallback to UNK)\n",
    "    token_ids = [word2idx.get(tok, word2idx[\"UNK\"]) for tok in tokens]\n",
    "    \n",
    "    # Pad to max_len\n",
    "    padded = token_ids + [word2idx[\"PAD\"]] * (max_len - len(token_ids))\n",
    "    device = next(model.parameters()).device\n",
    "    input_tensor = torch.tensor([padded[:max_len]], dtype=torch.long).to(device)    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "        pred_ids = torch.argmax(logits, dim=-1)[0].tolist()\n",
    "        pred_tags = [idx2tag[i] for i in pred_ids[:len(tokens)]]\n",
    "\n",
    "    return list(zip(tokens, pred_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57299abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('he', 'I-EDU'), (' ', 'B-EDU'), ('grad', 'I-EDU'), ('uat', 'I-HSK'), ('ed', 'I-HSK'), (' ', 'I-EDU'), ('from', 'I-EDU'), (' ', 'B-ORG'), ('s', 'I-EDU'), ('tanf', 'I-ORG'), ('or', 'I-HSK'), ('##d', 'B-ORG'), (' ', 'B-EDU'), ('university', 'I-EDU'), (' ', 'I-EDU'), ('in', 'I-EDU'), (' ', 'B-HSK'), ('2015', 'I-EDU'), (' ', 'B-ORG'), ('with', 'I-EDU'), (' ', 'B-ORG'), ('a', 'I-EDU'), (' ', 'B-HSK'), ('d', 'I-EDU'), ('eg', 'B-HSK'), ('re', 'B-JOB'), ('##e', 'B-ORG'), (' ', 'I-ORG'), ('in', 'I-EDU'), (' ', 'B-HSK'), ('comp', 'I-ORG'), ('ut', 'B-JOB'), ('##er', 'I-ORG'), (' ', 'B-ORG'), ('sci', 'I-HSK'), ('enc', 'I-ORG'), ('e', 'I-ORG'), ('.', 'I-EDU')]\n"
     ]
    }
   ],
   "source": [
    "text = \"He graduated from Stanford University in 2015 with a degree in Computer Science.\"\n",
    "print(predict_sentence(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15078f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict new text here : \n",
    "text = \"Your Text Here.\"\n",
    "print(predict_sentence(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
